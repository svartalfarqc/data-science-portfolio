{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "toxicity-first-keras-lstm kaggle release 1 BACKUP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "NOk4vnvizaEe"
      ],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KrBpvZj-_DOV",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "# Toxicity // First Keras LSTM\n",
        "Project: [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nxgnU5hezaC6"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Based on [Simple LSTM kernel](https://www.kaggle.com/thousandvoices/simple-lstm). Credit to @thousandvoice for base model and preprocessing.\n",
        "\n",
        "I attempted to improve with:\n",
        "\n",
        "*   Preprocessing steps for contractions and other vocabulary treatment\n",
        "\n",
        "* K-fold cross-validation\n",
        "\n",
        "My objective is not to win. This is my first Keras LSTM and, incidentally, my first serious neural network implementation.\n",
        "\n",
        "A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.\n",
        "\n",
        "LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. LSTMs were developed to deal with the exploding and vanishing gradient problems that can be encountered when training traditional RNNs.\n",
        "\n",
        "Q: So... why are we doing this if there is no time information here?\n",
        "\n",
        "Also, this guy says we should drop them entirely: https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0\n",
        "\n",
        "Go figure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3AaqRDu3SF-",
        "colab_type": "text"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "90JfVj3g_DOX"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-qpaA0Wc_DOY",
        "slideshow": {
          "slide_type": "skip"
        },
        "outputId": "564bc668-be23-44f2-c675-56578f5bbf5f",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Import libraries\n",
        "# MAIN\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "import math\n",
        "import seaborn as sns\n",
        "import operator\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\n",
        "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import LearningRateScheduler"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFJ51PyUbIoF",
        "colab_type": "text"
      },
      "source": [
        "## Lookups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDvJPyWjS--1",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "SPECIAL_CHARS_MAPPING = {\"_\":\" \", \"`\":\" \"}\n",
        "SPECIAL_CHARS = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
        "\n",
        "def clean_special_chars(text):\n",
        "    for p in SPECIAL_CHARS_MAPPING:\n",
        "        text = text.replace(p, SPECIAL_CHARS_MAPPING[p])    \n",
        "    for p in SPECIAL_CHARS:\n",
        "        text = text.replace(p, f' {p} ')     \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40WxByRRrEb7",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "CONTRACTION_LOOKUP_EN = {\"ain't\": \"is not\"\n",
        "                      , \"aren't\": \"are not\"\n",
        "                      ,\"can't\": \"cannot\"\n",
        "                      , \"'cause\": \"because\"\n",
        "                      , \"could've\": \"could have\"\n",
        "                      , \"couldn't\": \"could not\", \"didn't\": \"did not\"\n",
        "                      ,  \"doesn't\": \"does not\", \"don't\": \"do not\"\n",
        "                      , \"hadn't\": \"had not\", \"hasn't\": \"has not\"\n",
        "                      , \"haven't\": \"have not\", \"he'd\": \"he would\"\n",
        "                      ,\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\"\n",
        "                      , \"how'd'y\": \"how do you\", \"how'll\": \"how will\"\n",
        "                      , \"how's\": \"how is\",  \"I'd\": \"I would\"\n",
        "                      , \"I'd've\": \"I would have\", \"I'll\": \"I will\"\n",
        "                      , \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\"\n",
        "                      , \"i'd\": \"i would\", \"i'd've\": \"i would have\"\n",
        "                         , \"i'll\": \"i will\",  \"i'll've\": \"i will have\"\n",
        "                         ,\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\"\n",
        "                         , \"it'd\": \"it would\", \"it'd've\": \"it would have\"\n",
        "                         , \"it'll\": \"it will\", \"it'll've\": \"it will have\"\n",
        "                         ,\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\"\n",
        "                         , \"mayn't\": \"may not\", \"might've\": \"might have\"\n",
        "                         ,\"mightn't\": \"might not\"\n",
        "                         ,\"mightn't've\": \"might not have\"\n",
        "                         , \"must've\": \"must have\", \"mustn't\": \"must not\"\n",
        "                         , \"mustn't've\": \"must not have\", \"needn't\": \"need not\"\n",
        "                         , \"needn't've\": \"need not have\"\n",
        "                         ,\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\"\n",
        "                         , \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\"\n",
        "                         , \"shan't've\": \"shall not have\", \"she'd\": \"she would\"\n",
        "                         , \"she'd've\": \"she would have\", \"she'll\": \"she will\"\n",
        "                         , \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\"\n",
        "                         , \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\"\n",
        "                         , \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\"\n",
        "                         ,\"that'd\": \"that would\", \"that'd've\": \"that would have\"\n",
        "                         , \"that's\": \"that is\", \"there'd\": \"there would\"\n",
        "                         , \"there'd've\": \"there would have\", \"there's\": \"there is\"\n",
        "                         , \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\"\n",
        "                         , \"they'll\": \"they will\", \"they'll've\": \"they will have\"\n",
        "                         , \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\"\n",
        "                         , \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\"\n",
        "                         , \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\"\n",
        "                         , \"what'll\": \"what will\", \"what'll've\": \"what will have\"\n",
        "                         , \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\"\n",
        "                         , \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\"\n",
        "                         , \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\"\n",
        "                         , \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\"\n",
        "                         , \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\"\n",
        "                         , \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\"\n",
        "                         , \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\"\n",
        "                         , \"y'all\": \"you all\", \"y'all'd\": \"you all would\"\n",
        "                         ,\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\"\n",
        "                         ,\"y'all've\": \"you all have\",\"you'd\": \"you would\"\n",
        "                         , \"you'd've\": \"you would have\", \"you'll\": \"you will\"\n",
        "                         , \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s9k36EurEb9",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def known_contractions(embed):\n",
        "    \"\"\"\n",
        "    Returns an array of contractions from the lookup that are found in an embedding matrix\n",
        "    \"\"\"\n",
        "    known = []\n",
        "    for contract in CONTRACTION_LOOKUP_EN:\n",
        "        if contract in embed:\n",
        "            known.append(contract)\n",
        "    return known"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuKBJ5uXVvih",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "#PUNCT_MAPPING = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35aw09UbdtnV",
        "colab_type": "text"
      },
      "source": [
        "## Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv3Cw9vld45y",
        "colab_type": "code",
        "trusted": true,
        "colab": {},
        "outputId": "9bf9cbc1-c5d0-47ae-a3ad-65a2c928a0d9"
      },
      "source": [
        "import os\n",
        "print(os.listdir(\"../input\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['fasttext-crawl-300d-2m', 'jigsaw-unintended-bias-in-toxicity-classification']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "83H9oDj_bIoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
        "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSLfaJKAmEKB",
        "colab_type": "text"
      },
      "source": [
        "# Prepare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nDROc8VJmBU0"
      },
      "source": [
        "## Word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0uzFIZXVmBU0",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "# Assign coefficient to word\n",
        "def get_coefs(word, *arr):\n",
        "    return word, np.asarray(arr, dtype='float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cV-attBLmBU2",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "# Load embeddings from a file path and assign coefficients\n",
        "def load_embeddings(path):\n",
        "    with open(path) as f:\n",
        "        return dict(get_coefs(*line.strip().split(' ')) for line in f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ndh6Kz8emBU3",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "# FILES\n",
        "\n",
        "filepath_crawl = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n",
        "embeddings_crawl = load_embeddings(filepath_crawl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNVu49obrEbp",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmF5tQkMbIof",
        "colab_type": "text"
      },
      "source": [
        "## Configure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OTeevr2VbIog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_MODELS = 2\n",
        "BATCH_SIZE = 512\n",
        "LSTM_UNITS = 128\n",
        "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
        "EPOCHS = 4\n",
        "MAX_LEN = 220"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW1U4HaXeFDy",
        "colab_type": "text"
      },
      "source": [
        "## Text preprocessing and tokenizer\n",
        "\n",
        "This section is based on [this kernel](https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2) by [@theoviel](https://www.kaggle.com/theoviel), which itself is based on [this kernel](https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings) by [@Dieter](https://www.kaggle.com/christofhenkel).\n",
        "\n",
        "I added comments and tweaked a little bit.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ipAt6Jg-zaEX",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def preprocess(data):\n",
        "    \"\"\"\n",
        "    Preprocess an array of strings:\n",
        "    1) clean special characters\n",
        "    2) ...\n",
        "    3) profit!\n",
        "    \"\"\"\n",
        "\n",
        "    # clean special characters\n",
        "    data = data.astype(str).apply(lambda x: clean_special_chars(x))\n",
        "\n",
        "    return data\n",
        "\n",
        "x_train = preprocess(train['comment_text'])\n",
        "x_test = preprocess(test['comment_text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RHRrLJPmWeF6",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "# transform target into boolean\n",
        "y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
        "\n",
        "# auxiliary results\n",
        "# Q: why does it include the target?\n",
        "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um17YOY9XJO3",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H113gsUUMhsF",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "# Turn each text into either a sequence dictionary tokens (integers) or\n",
        "# a vector where the coefficient for each token could be binary, based on word count, based on tf-idf...\n",
        "# Q: what is that second part?\n",
        "tokenizer = text.Tokenizer()\n",
        "\n",
        "# tokenize both train and test data\n",
        "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
        "\n",
        "# Transform text in sequence of integers\n",
        "x_train = tokenizer.texts_to_sequences(x_train)\n",
        "x_test = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "# Pad sequences to the same length (padded with 0 by default)\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9ctMLluoN8q",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def build_matrix(word_index):\n",
        "    \"\"\"\n",
        "    Build embeddings matrix from train and test data\n",
        "    \n",
        "    @args: \n",
        "    - word_index = \n",
        "    \"\"\"\n",
        "  \n",
        "    # Use FastText Crawl only (for now)\n",
        "    embedding_index = embeddings_crawl\n",
        "    \n",
        "    # Q: why 300? the MAX_LEN is 220 no?\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "    \n",
        "    for word, i in word_index.items():\n",
        "        try:\n",
        "            embedding_matrix[i] = embedding_index[word]\n",
        "        except KeyError:\n",
        "            pass\n",
        "    return embedding_matrix\n",
        "\n",
        "embedding_matrix = build_matrix(tokenizer.word_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hIzVJdOo_DO3"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QUivhCmbIox",
        "colab_type": "text"
      },
      "source": [
        "## Configure model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FRSHT-ctbIoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_predictions = []\n",
        "weights = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BvzegugpzaDY",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "# Build model\n",
        "def build_model(embedding_matrix, num_aux_targets, loss_fn='binary_crossentropy', optimizer='adam'):\n",
        "    \n",
        "    # Create Input Layer\n",
        "    \n",
        "    # Q: Why is it a MAX_LEN dimensional vector?\n",
        "    words = Input(shape=(MAX_LEN,))\n",
        "    \n",
        "    # Feature Scaling\n",
        "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
        "    \n",
        "    # Dropout regularization to avoid overfitting\n",
        "    # How it works: at each iteration of the training, some neurons are\n",
        "    # randomly disabled to prevent them from being too dependent on each\n",
        "    # other when they learn their correlations (because we don't have the same configuration each time)\n",
        "    \n",
        "    # G: I guess this is used to avoid overfitting and too much correlation between words that are nearby???\n",
        "    \n",
        "    # This version performs the same function as Dropout, however it drops entire 1D feature maps instead of individual elements.\n",
        "    # If adjacent frames within feature maps are strongly correlated \n",
        "    # (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and \n",
        "    # will otherwise just result in an effective learning rate decrease.\n",
        "    # In this case, SpatialDropout1D will help promote independence between feature maps and should be used instead.\n",
        "    x = SpatialDropout1D(0.3)(x)\n",
        "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
        "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
        "\n",
        "    # In the last few years, experts have turned to global average pooling (GAP) layers to minimize overfitting \n",
        "    # by reducing the total number of parameters in the model. \n",
        "    # Similar to max pooling layers, GAP layers are used to reduce the spatial dimensions of a three-dimensional tensor. \n",
        "    \n",
        "    # Q: Why do we here use both a max pooling and an average pooling?\n",
        "    hidden = concatenate([\n",
        "        GlobalMaxPooling1D()(x),\n",
        "        GlobalAveragePooling1D()(x),\n",
        "    ])\n",
        "    \n",
        "    # Add two rectifier function hidden layers\n",
        "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
        "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
        "    \n",
        "    # Output layer\n",
        "    result = Dense(1, activation='sigmoid')(hidden)\n",
        "    \n",
        "    # Auxiliary results (categorization)\n",
        "    # ex. 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat'\n",
        "    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n",
        "    \n",
        "    # Q: How do \"auxiliary results\" in models come into play?\n",
        "    model = Model(inputs=words, outputs=[result, aux_result])\n",
        "    \n",
        "    # Binary: toxic or not.\n",
        "    # Q: So... how DO auxiliary results come into play?!\n",
        "    # adam is a popular stochastic gradient descent optimizer function\n",
        "    model.compile(loss=loss_fn, optimizer=optimizer, metrics = ['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EbLLiSORzaEa",
        "outputId": "62da950a-d6ff-4e70-bb19-fa4f09eab997",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1492
        }
      },
      "source": [
        "for model_idx in range(NUM_MODELS):\n",
        "    model = build_model(embedding_matrix, y_aux_train.shape[-1])\n",
        "    for global_epoch in range(EPOCHS):\n",
        "        \n",
        "        #start_time = time.time()\n",
        "\n",
        "        #print('Epoch {}/{} \\t starttime={:.2f}s'.format(\n",
        "        #      global_epoch + 1, EPOCHS, start_time))\n",
        "        \n",
        "        model.fit(\n",
        "            x_train,\n",
        "            [y_train, y_aux_train],\n",
        "            batch_size=BATCH_SIZE,\n",
        "            epochs=1,\n",
        "            verbose=2,\n",
        "            callbacks=[\n",
        "                LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "        #elapsed_time = time.time() - start_time\n",
        "        #print('Epoch {}/{} \\t time={:.2f}s'.format(\n",
        "        #      global_epoch + 1, EPOCHS, elapsed_time))\n",
        "        \n",
        "        checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n",
        "        weights.append(2 ** global_epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Epoch 1/1\n",
            " - 645s - loss: 0.2466 - dense_3_loss: 0.1374 - dense_4_loss: 0.1091 - dense_3_acc: 0.9465 - dense_4_acc: 0.8546\n",
            "Epoch 1/1\n",
            " - 642s - loss: 0.2236 - dense_3_loss: 0.1198 - dense_4_loss: 0.1038 - dense_3_acc: 0.9519 - dense_4_acc: 0.8549\n",
            "Epoch 1/1\n",
            " - 643s - loss: 0.2179 - dense_3_loss: 0.1151 - dense_4_loss: 0.1028 - dense_3_acc: 0.9535 - dense_4_acc: 0.8550\n",
            "Epoch 1/1\n",
            " - 644s - loss: 0.2141 - dense_3_loss: 0.1118 - dense_4_loss: 0.1023 - dense_3_acc: 0.9545 - dense_4_acc: 0.8550\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-OnjzfCL6BDz"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ryjML16PzaEc",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "predictions = np.average(checkpoint_predictions, weights=weights, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HcnMOJgHpira",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "# score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "roc_auc_score(y_train>0.5,oof_preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NOk4vnvizaEe"
      },
      "source": [
        "## Submit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WiBPszztzaEf",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "submission = pd.DataFrame.from_dict({\n",
        "    'id': test['id'],\n",
        "    'prediction': predictions\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hLmH5Bv8bIpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}