{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KrBpvZj-_DOV",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Toxicity // First Keras LSTM\n",
    "Project: [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nxgnU5hezaC6"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Based on [Simple LSTM kernel](https://www.kaggle.com/thousandvoices/simple-lstm). Credit to @thousandvoice for base model and preprocessing.\n",
    "\n",
    "I attempted to improve with:\n",
    "\n",
    "*   Preprocessing steps for contractions and other vocabulary treatment\n",
    "\n",
    "* K-fold cross-validation\n",
    "\n",
    "My objective is not to win. This is my first Keras LSTM and, incidentally, my first serious neural network implementation.\n",
    "\n",
    "A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.\n",
    "\n",
    "LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. LSTMs were developed to deal with the exploding and vanishing gradient problems that can be encountered when training traditional RNNs.\n",
    "\n",
    "Q: So... why are we doing this if there is no time information here?\n",
    "\n",
    "Also, this guy says we should drop them entirely: https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0\n",
    "\n",
    "Go figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s3AaqRDu3SF-"
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "90JfVj3g_DOX"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-qpaA0Wc_DOY",
    "outputId": "564bc668-be23-44f2-c675-56578f5bbf5f",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "# MAIN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import math\n",
    "import seaborn as sns\n",
    "import operator\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jFJ51PyUbIoF"
   },
   "source": [
    "## Lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XDvJPyWjS--1"
   },
   "outputs": [],
   "source": [
    "SPECIAL_CHARS_MAPPING = {\"_\":\" \", \"`\":\" \"}\n",
    "SPECIAL_CHARS = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "\n",
    "def clean_special_chars(text):\n",
    "    for p in SPECIAL_CHARS_MAPPING:\n",
    "        text = text.replace(p, SPECIAL_CHARS_MAPPING[p])    \n",
    "    for p in SPECIAL_CHARS:\n",
    "        text = text.replace(p, f' {p} ')     \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40WxByRRrEb7"
   },
   "outputs": [],
   "source": [
    "CONTRACTION_LOOKUP_EN = {\"ain't\": \"is not\"\n",
    "                      , \"aren't\": \"are not\"\n",
    "                      ,\"can't\": \"cannot\"\n",
    "                      , \"'cause\": \"because\"\n",
    "                      , \"could've\": \"could have\"\n",
    "                      , \"couldn't\": \"could not\", \"didn't\": \"did not\"\n",
    "                      ,  \"doesn't\": \"does not\", \"don't\": \"do not\"\n",
    "                      , \"hadn't\": \"had not\", \"hasn't\": \"has not\"\n",
    "                      , \"haven't\": \"have not\", \"he'd\": \"he would\"\n",
    "                      ,\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\"\n",
    "                      , \"how'd'y\": \"how do you\", \"how'll\": \"how will\"\n",
    "                      , \"how's\": \"how is\",  \"I'd\": \"I would\"\n",
    "                      , \"I'd've\": \"I would have\", \"I'll\": \"I will\"\n",
    "                      , \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\"\n",
    "                      , \"i'd\": \"i would\", \"i'd've\": \"i would have\"\n",
    "                         , \"i'll\": \"i will\",  \"i'll've\": \"i will have\"\n",
    "                         ,\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\"\n",
    "                         , \"it'd\": \"it would\", \"it'd've\": \"it would have\"\n",
    "                         , \"it'll\": \"it will\", \"it'll've\": \"it will have\"\n",
    "                         ,\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\"\n",
    "                         , \"mayn't\": \"may not\", \"might've\": \"might have\"\n",
    "                         ,\"mightn't\": \"might not\"\n",
    "                         ,\"mightn't've\": \"might not have\"\n",
    "                         , \"must've\": \"must have\", \"mustn't\": \"must not\"\n",
    "                         , \"mustn't've\": \"must not have\", \"needn't\": \"need not\"\n",
    "                         , \"needn't've\": \"need not have\"\n",
    "                         ,\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\"\n",
    "                         , \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\"\n",
    "                         , \"shan't've\": \"shall not have\", \"she'd\": \"she would\"\n",
    "                         , \"she'd've\": \"she would have\", \"she'll\": \"she will\"\n",
    "                         , \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\"\n",
    "                         , \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\"\n",
    "                         , \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\"\n",
    "                         ,\"that'd\": \"that would\", \"that'd've\": \"that would have\"\n",
    "                         , \"that's\": \"that is\", \"there'd\": \"there would\"\n",
    "                         , \"there'd've\": \"there would have\", \"there's\": \"there is\"\n",
    "                         , \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\"\n",
    "                         , \"they'll\": \"they will\", \"they'll've\": \"they will have\"\n",
    "                         , \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\"\n",
    "                         , \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\"\n",
    "                         , \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\"\n",
    "                         , \"what'll\": \"what will\", \"what'll've\": \"what will have\"\n",
    "                         , \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\"\n",
    "                         , \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\"\n",
    "                         , \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\"\n",
    "                         , \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\"\n",
    "                         , \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\"\n",
    "                         , \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\"\n",
    "                         , \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\"\n",
    "                         , \"y'all\": \"you all\", \"y'all'd\": \"you all would\"\n",
    "                         ,\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\"\n",
    "                         ,\"y'all've\": \"you all have\",\"you'd\": \"you would\"\n",
    "                         , \"you'd've\": \"you would have\", \"you'll\": \"you will\"\n",
    "                         , \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2s9k36EurEb9"
   },
   "outputs": [],
   "source": [
    "def known_contractions(embed):\n",
    "    \"\"\"\n",
    "    Returns an array of contractions from the lookup that are found in an embedding matrix\n",
    "    \"\"\"\n",
    "    known = []\n",
    "    for contract in CONTRACTION_LOOKUP_EN:\n",
    "        if contract in embed:\n",
    "            known.append(contract)\n",
    "    return known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HuKBJ5uXVvih"
   },
   "outputs": [],
   "source": [
    "#PUNCT_MAPPING = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35aw09UbdtnV"
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xv3Cw9vld45y",
    "outputId": "9bf9cbc1-c5d0-47ae-a3ad-65a2c928a0d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-crawl-300d-2m', 'jigsaw-unintended-bias-in-toxicity-classification']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "83H9oDj_bIoV"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZSLfaJKAmEKB"
   },
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nDROc8VJmBU0"
   },
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0uzFIZXVmBU0"
   },
   "outputs": [],
   "source": [
    "# Assign coefficient to word\n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cV-attBLmBU2"
   },
   "outputs": [],
   "source": [
    "# Load embeddings from a file path and assign coefficients\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ndh6Kz8emBU3"
   },
   "outputs": [],
   "source": [
    "# FILES\n",
    "\n",
    "filepath_crawl = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n",
    "embeddings_crawl = load_embeddings(filepath_crawl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNVu49obrEbp"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qmF5tQkMbIof"
   },
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OTeevr2VbIog"
   },
   "outputs": [],
   "source": [
    "NUM_MODELS = 2\n",
    "BATCH_SIZE = 512\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "EPOCHS = 4\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jW1U4HaXeFDy"
   },
   "source": [
    "## Text preprocessing and tokenizer\n",
    "\n",
    "This section is based on [this kernel](https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2) by [@theoviel](https://www.kaggle.com/theoviel), which itself is based on [this kernel](https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings) by [@Dieter](https://www.kaggle.com/christofhenkel).\n",
    "\n",
    "I added comments and tweaked a little bit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ipAt6Jg-zaEX"
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    \"\"\"\n",
    "    Preprocess an array of strings:\n",
    "    1) clean special characters\n",
    "    2) ...\n",
    "    3) profit!\n",
    "    \"\"\"\n",
    "\n",
    "    # clean special characters\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars(x))\n",
    "\n",
    "    return data\n",
    "\n",
    "x_train = preprocess(train['comment_text'])\n",
    "x_test = preprocess(test['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RHRrLJPmWeF6"
   },
   "outputs": [],
   "source": [
    "# transform target into boolean\n",
    "y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
    "\n",
    "# auxiliary results\n",
    "# Q: why does it include the target?\n",
    "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Um17YOY9XJO3"
   },
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H113gsUUMhsF"
   },
   "outputs": [],
   "source": [
    "# Turn each text into either a sequence dictionary tokens (integers) or\n",
    "# a vector where the coefficient for each token could be binary, based on word count, based on tf-idf...\n",
    "# Q: what is that second part?\n",
    "tokenizer = text.Tokenizer()\n",
    "\n",
    "# tokenize both train and test data\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "\n",
    "# Transform text in sequence of integers\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "# Pad sequences to the same length (padded with 0 by default)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o9ctMLluoN8q"
   },
   "outputs": [],
   "source": [
    "def build_matrix(word_index):\n",
    "    \"\"\"\n",
    "    Build embeddings matrix from train and test data\n",
    "    \n",
    "    @args: \n",
    "    - word_index = \n",
    "    \"\"\"\n",
    "  \n",
    "    # Use FastText Crawl only (for now)\n",
    "    embedding_index = embeddings_crawl\n",
    "    \n",
    "    # Q: why 300? the MAX_LEN is 220 no?\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = build_matrix(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hIzVJdOo_DO3"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8QUivhCmbIox"
   },
   "source": [
    "## Configure model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FRSHT-ctbIoz"
   },
   "outputs": [],
   "source": [
    "checkpoint_predictions = []\n",
    "weights = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BvzegugpzaDY"
   },
   "outputs": [],
   "source": [
    "# Build model\n",
    "def build_model(embedding_matrix, num_aux_targets, loss_fn='binary_crossentropy', optimizer='adam'):\n",
    "    \n",
    "    # Create Input Layer\n",
    "    \n",
    "    # Q: Why is it a MAX_LEN dimensional vector?\n",
    "    words = Input(shape=(MAX_LEN,))\n",
    "    \n",
    "    # Feature Scaling\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    \n",
    "    # Dropout regularization to avoid overfitting\n",
    "    # How it works: at each iteration of the training, some neurons are\n",
    "    # randomly disabled to prevent them from being too dependent on each\n",
    "    # other when they learn their correlations (because we don't have the same configuration each time)\n",
    "    \n",
    "    # G: I guess this is used to avoid overfitting and too much correlation between words that are nearby???\n",
    "    \n",
    "    # This version performs the same function as Dropout, however it drops entire 1D feature maps instead of individual elements.\n",
    "    # If adjacent frames within feature maps are strongly correlated \n",
    "    # (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and \n",
    "    # will otherwise just result in an effective learning rate decrease.\n",
    "    # In this case, SpatialDropout1D will help promote independence between feature maps and should be used instead.\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    # In the last few years, experts have turned to global average pooling (GAP) layers to minimize overfitting \n",
    "    # by reducing the total number of parameters in the model. \n",
    "    # Similar to max pooling layers, GAP layers are used to reduce the spatial dimensions of a three-dimensional tensor. \n",
    "    \n",
    "    # Q: Why do we here use both a max pooling and an average pooling?\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    \n",
    "    # Add two rectifier function hidden layers\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    \n",
    "    # Output layer\n",
    "    result = Dense(1, activation='sigmoid')(hidden)\n",
    "    \n",
    "    # Auxiliary results (categorization)\n",
    "    # ex. 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat'\n",
    "    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n",
    "    \n",
    "    # Q: How do \"auxiliary results\" in models come into play?\n",
    "    model = Model(inputs=words, outputs=[result, aux_result])\n",
    "    \n",
    "    # Binary: toxic or not.\n",
    "    # Q: So... how DO auxiliary results come into play?!\n",
    "    # adam is a popular stochastic gradient descent optimizer function\n",
    "    model.compile(loss=loss_fn, optimizer=optimizer, metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1492
    },
    "colab_type": "code",
    "id": "EbLLiSORzaEa",
    "outputId": "62da950a-d6ff-4e70-bb19-fa4f09eab997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/1\n",
      " - 645s - loss: 0.2466 - dense_3_loss: 0.1374 - dense_4_loss: 0.1091 - dense_3_acc: 0.9465 - dense_4_acc: 0.8546\n",
      "Epoch 1/1\n",
      " - 642s - loss: 0.2236 - dense_3_loss: 0.1198 - dense_4_loss: 0.1038 - dense_3_acc: 0.9519 - dense_4_acc: 0.8549\n",
      "Epoch 1/1\n",
      " - 643s - loss: 0.2179 - dense_3_loss: 0.1151 - dense_4_loss: 0.1028 - dense_3_acc: 0.9535 - dense_4_acc: 0.8550\n",
      "Epoch 1/1\n",
      " - 644s - loss: 0.2141 - dense_3_loss: 0.1118 - dense_4_loss: 0.1023 - dense_3_acc: 0.9545 - dense_4_acc: 0.8550\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "for model_idx in range(NUM_MODELS):\n",
    "    model = build_model(embedding_matrix, y_aux_train.shape[-1])\n",
    "    for global_epoch in range(EPOCHS):\n",
    "        \n",
    "        #start_time = time.time()\n",
    "\n",
    "        #print('Epoch {}/{} \\t starttime={:.2f}s'.format(\n",
    "        #      global_epoch + 1, EPOCHS, start_time))\n",
    "        \n",
    "        model.fit(\n",
    "            x_train,\n",
    "            [y_train, y_aux_train],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=1,\n",
    "            verbose=2,\n",
    "            callbacks=[\n",
    "                LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        #elapsed_time = time.time() - start_time\n",
    "        #print('Epoch {}/{} \\t time={:.2f}s'.format(\n",
    "        #      global_epoch + 1, EPOCHS, elapsed_time))\n",
    "        \n",
    "        checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n",
    "        weights.append(2 ** global_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-OnjzfCL6BDz"
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ryjML16PzaEc"
   },
   "outputs": [],
   "source": [
    "predictions = np.average(checkpoint_predictions, weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HcnMOJgHpira"
   },
   "outputs": [],
   "source": [
    "# score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train>0.5,oof_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOk4vnvizaEe"
   },
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WiBPszztzaEf"
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test['id'],\n",
    "    'prediction': predictions\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hLmH5Bv8bIpD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "NOk4vnvizaEe"
   ],
   "name": "toxicity-first-keras-lstm kaggle release 1 BACKUP.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
