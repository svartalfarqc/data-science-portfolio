{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KrBpvZj-_DOV",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Toxicity // Second Keras LSTM\n",
    "Project: [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nxgnU5hezaC6"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Second version of my initial kernel, which was stripped down to its simplest form because I ran out of memory on the kernel for reasons still unknown.\n",
    "\n",
    "This second step was meant to be an iteration with more preprocessing: contraction.\n",
    "\n",
    "However, the second version does not rank at all and at the time of writing, I do not understand why.\n",
    "\n",
    "Based on [Simple LSTM kernel](https://www.kaggle.com/thousandvoices/simple-lstm). Credit to @thousandvoice for base model and initial preprocessing.\n",
    "\n",
    "My objective is not to win. This is my first real world Keras implementation.\n",
    "\n",
    "I updated / generalized some code, added some comments, added preprocessing steps, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General:\n",
    "* Do some EDA\n",
    "* GridSearch (while avoiding memory problems...)\n",
    "* Limit number of words used in the neural network to avoid overfitting\n",
    "* ...\n",
    "\n",
    "\n",
    "Pre-processing:\n",
    "* Do something with words not found in vocabulary and sort out:\n",
    " * first names, last names, places, etc.\n",
    " * spelling mistakes (fix them)\n",
    " * acronyms (replace with words)\n",
    " * etc.\n",
    " \n",
    "* Try:\n",
    " * weighted average of embeddings\n",
    " * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s3AaqRDu3SF-"
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "90JfVj3g_DOX"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-qpaA0Wc_DOY",
    "outputId": "ea0c7d5b-4855-43e4-9bc9-a0d6f4c915b1",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "# MAIN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import math\n",
    "import seaborn as sns\n",
    "import operator\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XDvJPyWjS--1"
   },
   "outputs": [],
   "source": [
    "SPECIAL_CHARS_MAPPING = {\"_\":\" \", \"`\":\" \"}\n",
    "SPECIAL_CHARS = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "\n",
    "def clean_special_chars(text):\n",
    "    for p in SPECIAL_CHARS_MAPPING:\n",
    "        text = text.replace(p, SPECIAL_CHARS_MAPPING[p])    \n",
    "    for p in SPECIAL_CHARS:\n",
    "        text = text.replace(p, f' {p} ')     \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40WxByRRrEb7"
   },
   "outputs": [],
   "source": [
    "CONTRACTION_LOOKUP_EN = {\"ain't\": \"is not\"\n",
    "                      , \"aren't\": \"are not\"\n",
    "                      ,\"can't\": \"cannot\"\n",
    "                      , \"'cause\": \"because\"\n",
    "                      , \"could've\": \"could have\"\n",
    "                      , \"couldn't\": \"could not\", \"didn't\": \"did not\"\n",
    "                      ,  \"doesn't\": \"does not\", \"don't\": \"do not\"\n",
    "                      , \"hadn't\": \"had not\", \"hasn't\": \"has not\"\n",
    "                      , \"haven't\": \"have not\", \"he'd\": \"he would\"\n",
    "                      ,\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\"\n",
    "                      , \"how'd'y\": \"how do you\", \"how'll\": \"how will\"\n",
    "                      , \"how's\": \"how is\",  \"I'd\": \"I would\"\n",
    "                      , \"I'd've\": \"I would have\", \"I'll\": \"I will\"\n",
    "                      , \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\"\n",
    "                      , \"i'd\": \"i would\", \"i'd've\": \"i would have\"\n",
    "                         , \"i'll\": \"i will\",  \"i'll've\": \"i will have\"\n",
    "                         ,\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\"\n",
    "                         , \"it'd\": \"it would\", \"it'd've\": \"it would have\"\n",
    "                         , \"it'll\": \"it will\", \"it'll've\": \"it will have\"\n",
    "                         ,\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\"\n",
    "                         , \"mayn't\": \"may not\", \"might've\": \"might have\"\n",
    "                         ,\"mightn't\": \"might not\"\n",
    "                         ,\"mightn't've\": \"might not have\"\n",
    "                         , \"must've\": \"must have\", \"mustn't\": \"must not\"\n",
    "                         , \"mustn't've\": \"must not have\", \"needn't\": \"need not\"\n",
    "                         , \"needn't've\": \"need not have\"\n",
    "                         ,\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\"\n",
    "                         , \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\"\n",
    "                         , \"shan't've\": \"shall not have\", \"she'd\": \"she would\"\n",
    "                         , \"she'd've\": \"she would have\", \"she'll\": \"she will\"\n",
    "                         , \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\"\n",
    "                         , \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\"\n",
    "                         , \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\"\n",
    "                         ,\"that'd\": \"that would\", \"that'd've\": \"that would have\"\n",
    "                         , \"that's\": \"that is\", \"there'd\": \"there would\"\n",
    "                         , \"there'd've\": \"there would have\", \"there's\": \"there is\"\n",
    "                         , \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\"\n",
    "                         , \"they'll\": \"they will\", \"they'll've\": \"they will have\"\n",
    "                         , \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\"\n",
    "                         , \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\"\n",
    "                         , \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\"\n",
    "                         , \"what'll\": \"what will\", \"what'll've\": \"what will have\"\n",
    "                         , \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\"\n",
    "                         , \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\"\n",
    "                         , \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\"\n",
    "                         , \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\"\n",
    "                         , \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\"\n",
    "                         , \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\"\n",
    "                         , \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\"\n",
    "                         , \"y'all\": \"you all\", \"y'all'd\": \"you all would\"\n",
    "                         ,\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\"\n",
    "                         ,\"y'all've\": \"you all have\",\"you'd\": \"you would\"\n",
    "                         , \"you'd've\": \"you would have\", \"you'll\": \"you will\"\n",
    "                         , \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2s9k36EurEb9"
   },
   "outputs": [],
   "source": [
    "def known_contractions(embed):\n",
    "    \"\"\"\n",
    "    Returns an array of contractions from the lookup that are found in an embedding matrix\n",
    "    \"\"\"\n",
    "    known = []\n",
    "    for contract in CONTRACTION_LOOKUP_EN:\n",
    "        if contract in embed:\n",
    "            known.append(contract)\n",
    "    return known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35aw09UbdtnV"
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xv3Cw9vld45y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-crawl-300d-2m', 'jigsaw-unintended-bias-in-toxicity-classification']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZSLfaJKAmEKB"
   },
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nDROc8VJmBU0"
   },
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0uzFIZXVmBU0"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    \"\"\"\n",
    "    Assign coefficient to word\n",
    "    \"\"\"\n",
    "    return word, np.asarray(arr, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cV-attBLmBU2"
   },
   "outputs": [],
   "source": [
    "def load_embeddings(path):\n",
    "    \"\"\"\n",
    "    # Load embeddings from a file path and assign coefficients\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ndh6Kz8emBU3"
   },
   "outputs": [],
   "source": [
    "# FILES\n",
    "\n",
    "filepath_crawl = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n",
    "embeddings_crawl = load_embeddings(filepath_crawl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNVu49obrEbp"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MODELS = 2\n",
    "BATCH_SIZE = 512\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "EPOCHS = 4\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jW1U4HaXeFDy"
   },
   "source": [
    "## Text preprocessing and tokenizer\n",
    "\n",
    "This section is based on [this kernel](https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2) by [@theoviel](https://www.kaggle.com/theoviel), which itself is based on [this kernel](https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings) by [@Dieter](https://www.kaggle.com/christofhenkel).\n",
    "\n",
    "I added comments and tweaked a little bit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the missing contractions\n",
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ipAt6Jg-zaEX"
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    \"\"\"\n",
    "    Preprocess an array of strings:\n",
    "    1) clean special characters\n",
    "    2) clean contractions\n",
    "    3) ...\n",
    "    4) ... to be continued\n",
    "    \"\"\"\n",
    "\n",
    "    # clean special characters\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars(x))\n",
    "    \n",
    "    # clean contractions\n",
    "    data = data.astype(str).apply(lambda x: clean_contractions(x, CONTRACTION_LOOKUP_EN))\n",
    "    return data\n",
    "\n",
    "x_train = preprocess(train['comment_text'])\n",
    "x_test = preprocess(test['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RHRrLJPmWeF6"
   },
   "outputs": [],
   "source": [
    "# transform target into boolean\n",
    "y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
    "\n",
    "# auxiliary results\n",
    "# Q: why does it include the target?\n",
    "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Um17YOY9XJO3"
   },
   "source": [
    "## Tokenize\n",
    "Transform text into integer tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H113gsUUMhsF"
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "\n",
    "# tokenize both train and test data\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "\n",
    "# Transform text in sequence of integers\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "# Pad sequences to the same length (padded with 0 by default)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o9ctMLluoN8q"
   },
   "outputs": [],
   "source": [
    "def build_matrix(word_index):\n",
    "    \"\"\"\n",
    "    Build embeddings matrix from train and test data\n",
    "    \n",
    "    @args: word_index\n",
    "    \"\"\"\n",
    "  \n",
    "    # Use FastText Crawl only for memory purposes\n",
    "    embedding_index = embeddings_crawl\n",
    "    \n",
    "    # create matrix\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = build_matrix(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hIzVJdOo_DO3"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BvzegugpzaDY"
   },
   "outputs": [],
   "source": [
    "def build_model(embedding_matrix, num_aux_targets, loss_fn='binary_crossentropy', optimizer='adam'):\n",
    "    \n",
    "    # Create Input Layer\n",
    "    words = Input(shape=(MAX_LEN,))\n",
    "    \n",
    "    # Feature Scaling\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    \n",
    "    # Dropout regularization to avoid overfitting\n",
    "    # How it works: at each iteration of the training, some neurons are\n",
    "    # randomly disabled to prevent them from being too dependent on each\n",
    "    # other when they learn their correlations (because we don't have the same configuration each time)\n",
    "    \n",
    "    # This version performs the same function as Dropout, however it drops entire 1D feature maps instead of individual elements.\n",
    "    # If adjacent frames within feature maps are strongly correlated \n",
    "    # (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and \n",
    "    # will otherwise just result in an effective learning rate decrease.\n",
    "    # In this case, SpatialDropout1D will help promote independence between feature maps and should be used instead.\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    # In the last few years, experts have turned to global average pooling (GAP) layers to minimize overfitting \n",
    "    # by reducing the total number of parameters in the model. \n",
    "    # Similar to max pooling layers, GAP layers are used to reduce the spatial dimensions of a three-dimensional tensor. \n",
    "    \n",
    "    # Q: Unclear on why we are using both here.\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    \n",
    "    # Add two rectifier function hidden layers\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    \n",
    "    # Output layer\n",
    "    result = Dense(1, activation='sigmoid')(hidden)\n",
    "    \n",
    "    # Auxiliary results (categorization)\n",
    "    # ex. 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat'\n",
    "    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n",
    "    \n",
    "    # Create model, including auxiliary resuts\n",
    "    model = Model(inputs=words, outputs=[result, aux_result])\n",
    "    \n",
    "    # Binary: toxic or not.\n",
    "    model.compile(loss=loss_fn, optimizer=optimizer, metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1492
    },
    "colab_type": "code",
    "id": "EbLLiSORzaEa",
    "outputId": "62da950a-d6ff-4e70-bb19-fa4f09eab997"
   },
   "outputs": [],
   "source": [
    "checkpoint_predictions = []\n",
    "weights = []\n",
    "\n",
    "for model_idx in range(NUM_MODELS):\n",
    "    model = build_model(embedding_matrix, y_aux_train.shape[-1])\n",
    "    for global_epoch in range(EPOCHS):\n",
    "        \n",
    "        #start_time = time.time()\n",
    "\n",
    "        #print('Epoch {}/{} \\t starttime={:.2f}s'.format(\n",
    "        #      global_epoch + 1, EPOCHS, start_time))\n",
    "        \n",
    "        model.fit(\n",
    "            x_train,\n",
    "            [y_train, y_aux_train],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=1,\n",
    "            verbose=2,\n",
    "            callbacks=[\n",
    "                LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        #elapsed_time = time.time() - start_time\n",
    "        #print('Epoch {}/{} \\t time={:.2f}s'.format(\n",
    "        #      global_epoch + 1, EPOCHS, elapsed_time))\n",
    "        \n",
    "        checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n",
    "        weights.append(2 ** global_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-OnjzfCL6BDz"
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "colab_type": "code",
    "id": "ryjML16PzaEc",
    "outputId": "6f313ee4-7d94-466c-fc3d-347b378e7f81"
   },
   "outputs": [],
   "source": [
    "predictions = np.average(checkpoint_predictions, weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOk4vnvizaEe"
   },
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WiBPszztzaEf"
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test['id'],\n",
    "    'prediction': predictions\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "NOk4vnvizaEe"
   ],
   "name": "toxicity-first-keras-lstm COLAB.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
